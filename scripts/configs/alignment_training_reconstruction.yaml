is_query_positive_alignment: false
is_reconstruct: true
langs:
- en
max_seq_length: 512
neg_per_sample: 32
num_workers: 8
number_training_samples: 500000
pos_per_sample: 3
use_retrieval_data_only: false
attn_implementation: flash_attention_2
connection_type: ff
dropout: 0.1
encoder_backbone_type: mistral
encoder_lora_name: null
encoder_name_or_path: mistralai/Mistral-7B-Instruct-v0.3
is_freeze_universal_learner: false
loar_r: 16
lora_alpha: 32
num_added_tokens: 0
universal_learner_backbone_type: xlm-r
universal_learner_lora_name: universal_learner_lora
universal_learner_name_or_path: FacebookAI/xlm-roberta-large
activation_checkpointing: true
checkpoint_dir: output/xlm-mistral-aligment-phase1-reconstruct
checkpoint_file: null
checkpoint_interval: 1000
devices: 4
eval_batch_size: 128
gc_chunk_size: 16
global_batch_size: 256
grad_norm_clip: null
is_alignment: true
is_distance: true
learning_rate: 0.0002
log_interval: 1
logger_name: xlm-mistral
logger_type: wandb
loss_type: NTXentLoss
max_epochs: 5
max_steps: 10000
min_learning_rate: 1.0e-06
model_revision: xlm-mistral-alignment-phase1
nodes: 1
only_load_model: false
precision: bf16-mixed
quantization: false
seed: 777
sharding_strategy: shard_grad_op
strategy: fsdp
temperature: 0.05
use_cpu_offload: false
use_miner: true
warmpup_proportion: 0.1
weight_decay: 0.0
