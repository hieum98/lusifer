# Data args
langs:
  - en
use_retrieval_data_only: True
max_seq_length: 512
number_training_samples: 50_000
neg_per_sample: 32
pos_per_sample: 3
num_workers: 8

# Model args
universal_learner_name_or_path: google/mt5-xl
encoder_name_or_path: mistralai/Mistral-7B-Instruct-v0.3
universal_learner_backbone_type: t5
encoder_backbone_type: mistral
is_freeze_universal_learner: false
connection_type: ff
num_added_tokens: 0
# Lora args
encoder_lora_name: encoder_lora
universal_learner_lora_name: universal_learner_lora # Not configured, default is None
loar_r: 16
lora_alpha: 32
# Other args
dropout: 0.1
attn_implementation: 'flash_attention_2'

# Training args
is_alignment: false
seed: 777
model_revision: 't5-mistral.v0.1'
# Training strategy args
precision: 'bf16-mixed'
strategy: 'fsdp'
sharding_strategy: 'shard_grad_op'
activation_checkpointing: True
use_cpu_offload: False
quantization: False
# Loss args
loss_type: 'NTXentLoss'
temperature: 0.05
use_miner: True
is_distance: True
# Optimization args
global_batch_size: 64
eval_batch_size: 128
max_epochs: 1
max_steps: 100000
weight_decay: 0.0
warmpup_proportion: 0.1
# grad_norm_clip: 1.0 # Not configured, default is None
# Checkpoint args
checkpoint_interval: 1000
logger_type: 'wandb'
logger_name: 't5-mistral'



