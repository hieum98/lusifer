# Data args
is_query_positive_alignment: false
is_reconstruct: false
langs:
- en
mask_probability: 0.5
max_seq_length: 512
neg_per_sample: 32
num_workers: 8
number_training_samples: 5000
pos_per_sample: 3
use_retrieval_data_only: true

# Model args
attn_implementation: flash_attention_2
connection_type: ff
dropout: 0.1
encoder_backbone_type: nvidia/NV-Embed-v2
encoder_lora_name: encoder_lora
encoder_lora_target_modules:
- q_proj
- k_proj
- v_proj
- gate_proj
- down_proj
- up_proj
- o_proj
encoder_name_or_path: Hieuman/nvembed-v2-mistral
is_freeze_universal_learner: false
loar_r: 16
lora_alpha: 32
num_added_tokens: 1
universal_learner_backbone_type: xlm-r
universal_learner_lora_name: universal_lora
universal_learner_lora_target_modules: all
universal_learner_name_or_path: Hieuman/xlm-mistral-v0.1-align-phase1-universal-learner

# Training args
activation_checkpointing: true
checkpoint_dir: output/nv_embed-cl-phase1-lora.test3
checkpoint_file: output/Mistral-v0.1-completion-mask_0.5-add.v1.1/init_model_for_cl.ckpt
checkpoint_interval: 500
devices: 2
eval_batch_size: 128
gc_chunk_size: 2
global_batch_size: 32
grad_norm_clip: 1.0
is_alignment: false
is_distance: true
learning_rate: 5.0e-05
log_interval: 1
logger_name: xlm-mistral
logger_type: wandb
loss_type: NTXentLoss
max_epochs: 5
max_steps: 25000
min_learning_rate: 1.0e-07
model_revision: xlm-mistral-v0.1-cl
nodes: 1
only_load_model: true
precision: bf16-mixed
quantization: false
seed: 888
sharding_strategy: full_shard
strategy: fsdp
temperature: 0.05
use_cpu_offload: false
use_miner: true
warmpup_proportion: 0.1
weight_decay: 0.0
